program: train.py
name: sweep1
method: random  # Options: "bayes", "grid", etc.

metric:
  name: avg_reward  # Metric to optimize
  goal: maximize

parameters:
  learning_rate:
    values: [0.0001, 0.001, 0.01]
  batch_size:
    values: [8, 16, 32]
  eps_end:
    values: [0.01, 0.05, 0.1]
  discount_factor:
    value: 0.99
  replay_buffer_size:
    value: 100000
  update_freq:
    value: 100
  eps_start:
    value: 0.5
  schedule_duration:
    value: 15000
  num_episodes:
    value: 1000
  is_double_dqn:
    values: [False]  # Double DQN toggle
  is_dueling_dqn:
    values: [True]  # Dueling DQN toggle
  use_prioritized_replay:
    values: [False]  # Prioritized Replay toggle
  is_noisy_nets:
    values: [False]  # Noisy Networks toggle
  std_init:
    values: [0.1, 0.2, 0.3, 0.4, 0.5]  # Initial std for Noisy Nets
  is_distributional:
    values: [False]  # Distributional RL toggle
  num_atoms:
    values: [5, 21, 51]  # Number of atoms for Distributional RL

early_terminate:
  type: hyperband  # Aggressively terminate poorly performing runs
  min_iter: 10  # Allow at least 10 episodes before termination
  eta: 3  # Increase aggressiveness of termination
  s: 3  # Increase the number of brackets for better performance filtering
